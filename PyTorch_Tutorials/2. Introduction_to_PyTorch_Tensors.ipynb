{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to PyTorch Tensors (pytorch.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this second tutorial I will continue to explore PyTorch and see what Tensors are all about. This time i got a full running setup so i got started right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ source pytorch_env/Scripts/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `torch.empty()` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[-6.2937e+06,  1.0286e-42,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This created a 2-dimensional tensor of size 3x4. The values are not initialized and will vary depending on the state of memory. Data types are set by default (float). \n",
    "##### A 1-dimensional tensor is a vector\n",
    "##### A 2-dimensional tensor is a matrix\n",
    "##### To initialize tensors with some value use `torch` module with its factory methods. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Tensors and Seeding\n",
    "##### For reproducibility of results a random number generator's seed is helpful. Use `torch.manual_seed()` to obtain identical results for computations after setting the RNG's seed manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729) # setting the seed again will reset the random number generator and obtain the same random numbers\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Shapes: `torch.*_like()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.5062, 0.8469, 0.2588],\n",
      "         [0.2707, 0.4115, 0.6839]],\n",
      "\n",
      "        [[0.0703, 0.5105, 0.9451],\n",
      "         [0.2359, 0.1979, 0.3327]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another way to create a tensor is to specify data directly from a PyTorch collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7128],\n",
      "        [1.6180, 0.0073]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.7128], [1.61803, 0.0072897]])\n",
    "print(some_constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n"
     ]
    }
   ],
   "source": [
    "some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If there is data already present in a tuple or list, `torch.tensor()` is the most straightforward way to create a tensor. Nesting the collections will result in a multi-dimensional tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.3283,  0.2118, 18.4972],\n",
      "        [ 9.8370,  3.8937, 16.1945]], dtype=torch.float64)\n",
      "tensor([[18,  0, 18],\n",
      "        [ 9,  3, 16]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16) # 16-bit signed integer\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20. # 64-bit floating point\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32) # 32-bit signed integer\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math & Logic with PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "# tensor and scalar operations\n",
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# tensor and tensor operations\n",
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)\n",
    "\n",
    "# an important consideration when working with tensors is that the operations are element-wise\n",
    "# thus the tensors must have the same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Broadcasting\n",
    "###### Same rules to broadcasting semantics in NumPy ndarrays\n",
    "##### Broadcasting is a way to perform an operation between tensors that are similar in shape. In the following example, each row of the `rand` tensor is multiplied by the 1x4 tensor generating a new 2x4 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4485, 0.8740, 0.2526, 0.6923],\n",
      "        [0.7545, 0.7746, 0.2330, 0.8441]])\n",
      "tensor([[0.8971, 1.7480, 0.5051, 1.3846],\n",
      "        [1.5091, 1.5491, 0.4660, 1.6881]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4)*2)\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Deep Learning example: multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, returning a tensor with the same shape. \n",
    "##### Rules for broadcasting:\n",
    "###### - Each tensor must have at least one dimension (no empty tensors)\n",
    "###### - Comparing the dimension sizes of the two tensors, each dimension must be equal or one of the dimensions must be of size 1, or the dimension does not exist in one of the tensors. \n",
    "###### - Tensors of identical shape are trivially broadcastable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]],\n",
      "\n",
      "        [[0.9004, 0.3995],\n",
      "         [0.6324, 0.9464],\n",
      "         [0.0113, 0.5183]]])\n",
      "tensor([[[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]],\n",
      "\n",
      "        [[0.9807, 0.9807],\n",
      "         [0.6545, 0.6545],\n",
      "         [0.4144, 0.4144]]])\n",
      "tensor([[[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]],\n",
      "\n",
      "        [[0.0696, 0.4648],\n",
      "         [0.0696, 0.4648],\n",
      "         [0.0696, 0.4648]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Math with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.1018, 0.2531, 0.8823, 0.0156],\n",
      "        [0.0921, 0.0792, 0.3893, 0.6054]])\n",
      "tensor([[-0., 1., 1., -0.],\n",
      "        [1., 1., -0., -0.]])\n",
      "tensor([[-1.,  0.,  0., -1.],\n",
      "        [ 0.,  0., -1., -1.]])\n",
      "tensor([[-0.1018,  0.2531,  0.5000, -0.0156],\n",
      "        [ 0.0921,  0.0792, -0.3893, -0.5000]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.3285, 0.5655],\n",
      "        [0.0065, 0.7765]])\n",
      "tensor([[0.9856, 1.6966],\n",
      "        [0.0196, 2.3296]])\n",
      "torch.return_types.linalg_svd(\n",
      "U=tensor([[ 0.6345,  0.7729],\n",
      "        [ 0.7729, -0.6345]]),\n",
      "S=tensor([2.9475, 0.7677]),\n",
      "Vh=tensor([[ 0.2173,  0.9761],\n",
      "        [ 0.9761, -0.2173]]))\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.linalg.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.linalg.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.linalg.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altering Tensors in Place. \n",
    "##### When intermediate values of math operations are not needed and can be discarded, math functions have a version ith an appended underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
    "print(a)              # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))  # note the underscore\n",
    "print(b)              # b has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.3534, 0.7016],\n",
      "        [0.6826, 0.9413]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.3534, 1.7016],\n",
      "        [1.6826, 1.9413]])\n",
      "tensor([[1.3534, 1.7016],\n",
      "        [1.6826, 1.9413]])\n",
      "tensor([[0.3534, 0.7016],\n",
      "        [0.6826, 0.9413]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.1249, 0.4923],\n",
      "        [0.4660, 0.8861]])\n",
      "tensor([[0.1249, 0.4923],\n",
      "        [0.4660, 0.8861]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print('Before:')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b)) # the calling tensor a is the one modified in place.\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Many methods and functions have an `out` argument to specify a tensor to recieve the output (must have the correct shape and dtype)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.6517, 0.8166],\n",
      "        [0.5940, 0.7503]])\n",
      "tensor([[0.4363, 0.6339],\n",
      "        [0.3208, 0.4323]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copying Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "# Assigning a tensor to another variable does not create a copy of the tensor\n",
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "print(a)\n",
    "a[0][1] = 561  # we change a...\n",
    "print(b)       # ...and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To copy a tensor is better to use the `clone()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a # different objects in memory\n",
    "print(torch.eq(a, b)) # still have the same values\n",
    "a[0][1] = 561 # we change a...\n",
    "print(a)\n",
    "print(b) # ...and b is not altered this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When using `clone()`, if the source tensor has autograd enabled, then so will the clone. In case we wanted to avoid the cloned copy to track gradients we can use `.detach()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1811, 0.6962],\n",
      "        [0.8073, 0.2125]], requires_grad=True)\n",
      "tensor([[0.1811, 0.6962],\n",
      "        [0.8073, 0.2125]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.1811, 0.6962],\n",
      "        [0.8073, 0.2125]])\n",
      "tensor([[0.1811, 0.6962],\n",
      "        [0.8073, 0.2125]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving to Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do we move to faster hardware like CUDA, MPS, MTIA, or XPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator available!\n"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    print('Accelerator available!')\n",
    "else:\n",
    "    print('Sorry, CPU only!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To use the accelerator we must move the all the data needed for the specific computation to memory accessible by the device, in this case moving the data to the GPU. By default, new tensors are created on CPU so we have to specify whenever we want to use the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6815, 0.0556],\n",
      "        [0.0711, 0.4825]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.accelerator.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device=torch.accelerator.current_accelerator())\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To query the amount of accelerators we can use `torch.accelerator.device_count()`. If there are more than one accelerator, we can specify them by index: `device='cuda:0'`, `device='cuda:1'`, etc.\n",
    "##### It is convenient to create a device handle that can be passed to the tensors instead of using a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.4629, 0.2537],\n",
      "        [0.0404, 0.0187]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "my_device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When a tensor is created in a specific device it will live there until it is changed to another device with the `to()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2, 2) # creates the tensor on the CPU\n",
    "y = y.to(my_device) # moves the tensor to the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To do computation involving two or more tensors, all the tensors must be on the same device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Tensor Shapes: Changing number of dimensions\n",
    "##### Sometimes a model expects batches of input with a certain shape. For example if a model works on 3x226x266 (266px square image with 3 colors), when we load it and transform it we will get a tensor of shape (3, 266, 266). The model will expect input of shape (N, 3, 266, 266), where N is the number of images. We can extend the dimensions using `unsqueeze()` method. Now, if num_batches = 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0) # adds a dimension at the beginning. A new zeroth index/dimension\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `squeeze()` on the other hand, removes dimensions of size 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.0665]]]]])\n",
      "torch.Size([1, 20])\n",
      "tensor([[0.9433, 0.7006, 0.7753, 0.1242, 0.4711, 0.6189, 0.5459, 0.6927, 0.4136,\n",
      "         0.3063, 0.0856, 0.1923, 0.0667, 0.9678, 0.7519, 0.3056, 0.5590, 0.8477,\n",
      "         0.9800, 0.3000]])\n",
      "torch.Size([20])\n",
      "tensor([0.9433, 0.7006, 0.7753, 0.1242, 0.4711, 0.6189, 0.5459, 0.6927, 0.4136,\n",
      "        0.3063, 0.0856, 0.1923, 0.0667, 0.9678, 0.7519, 0.3056, 0.5590, 0.8477,\n",
      "        0.9800, 0.3000])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Examples of squeeze. Removes dimensions of size 1\n",
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)\n",
    "\n",
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0) # removes the first dimension\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.8337, 0.8337],\n",
      "         [0.2379, 0.2379],\n",
      "         [0.9515, 0.9515]],\n",
      "\n",
      "        [[0.8337, 0.8337],\n",
      "         [0.2379, 0.2379],\n",
      "         [0.9515, 0.9515]],\n",
      "\n",
      "        [[0.8337, 0.8337],\n",
      "         [0.2379, 0.2379],\n",
      "         [0.9515, 0.9515]],\n",
      "\n",
      "        [[0.8337, 0.8337],\n",
      "         [0.2379, 0.2379],\n",
      "         [0.9515, 0.9515]]])\n"
     ]
    }
   ],
   "source": [
    "# Another use of unsqueeze() is to ease broadcasting.\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)             # broadcasting works again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# in-place unsqueeze\n",
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To change the shape of a tensor radically while still preserving the number of elements and their contents we can use `reshape()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumPy Bridge.\n",
    "##### Existing ML models or scientific code with stored data in NumPy ndarrays, we can express the same data as PyTorch tensors with `torch.from_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2, 3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The other way around works with `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2980, 0.0100, 0.2972],\n",
      "        [0.9772, 0.9631, 0.0506]])\n",
      "[[0.2980491  0.00997722 0.2972157 ]\n",
      " [0.9772248  0.96310294 0.05055839]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2, 3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once finished this tutorial I feel like I understood the nature of tensors better than before. Very useful operations and methods, with a lot of potential. I like that data can be stored in them and that PyTorch allows the use of GPU on command for faster computations. Luckily my notebook can use CUDA so I will be able to use this feature in my models. I am thrilled about learning PyTorch and I'm excited to keep exploring the capabilities of this technology.\n",
    "##### - Arturo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
